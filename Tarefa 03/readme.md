# Tarefa 3: Explorando o Capítulo 6 – "Deep Learning with PyTorch"

**Nome:** Elisa Gabriela Machado de Lucena

**Link do artigo no medium:** https://medium.com/@elisa.lucena.127/ml-com-pytorch-adaptive-learning-rates-momentum-e-learning-rate-schedulers-211db8b9e367


Este repositório tem como objetivo explorar alguns conceitos do capítulo 6 do livro 'Deep Learning with PyTorch" de Daniel Godoy:

* EWMA meets gradients: Como as médias móveis exponencialmente ponderadas são usadas para suavizar os gradientes e seu impacto na atualização de parâmetros em otimizadores modernos;
* Adam: Funcionamento do otimizador adam, explicando o papel dos seus parâmetros na adaptação dos gradientes e na estabilidade do treinamento;
* Vizualização dos gradientes adaptados: Vizualizações que ilustram como os gradientes adaptados evoluem durante o treinamento, detacando os efeitos das médias móveis e da escala dos gradientes;
* SGD: Funcionamento do SGD básico e sua evolução para as variantes com momentum e nesterov, explicando a intuição por trás dessas melhorias;
* Learning rate schedulers: Como diferentes schedulers de learning rate podem ser aplicados, explicando a teoria e implementando exemplos práticos;
* Hora da prática: Desenvolvimento de um exemplo completo com base nos tópicos acima, integrando EWMA, adam, SGD e Learning rate schedulers.

