# Tarefa 3: Explorando o Cap√≠tulo 6 ‚Äì "Deep Learning with PyTorch"

##### Nome: Elisa Gabriela Machado de Lucena

* [Link do artigo no medium](https://medium.com/@elisa.lucena.127/ml-com-pytorch-adaptive-learning-rates-momentum-e-learning-rate-schedulers-211db8b9e367)

* [Notebook do desenvolvimento do trabalho](https://github.com/ElisaGabriela/Machine-Learning/blob/main/Tarefa%2003/Tarefa3.ipynb)


Este reposit√≥rio tem como objetivo explorar alguns conceitos do cap√≠tulo 6 do livro 'Deep Learning with PyTorch" de Daniel Godoy:

* EWMA meets gradients: Como as m√©dias m√≥veis exponencialmente ponderadas s√£o usadas para suavizar os gradientes e seu impacto na atualiza√ß√£o de par√¢metros em otimizadores modernos;
* Adam: Funcionamento do otimizador adam, explicando o papel dos seus par√¢metros na adapta√ß√£o dos gradientes e na estabilidade do treinamento;
* Vizualiza√ß√£o dos gradientes adaptados: Vizualiza√ß√µes que ilustram como os gradientes adaptados evoluem durante o treinamento, detacando os efeitos das m√©dias m√≥veis e da escala dos gradientes;
* SGD: Funcionamento do SGD b√°sico e sua evolu√ß√£o para as variantes com momentum e nesterov, explicando a intui√ß√£o por tr√°s dessas melhorias;
* Learning rate schedulers: Como diferentes schedulers de learning rate podem ser aplicados, explicando a teoria e implementando exemplos pr√°ticos;
* Hora da pr√°tica: Desenvolvimento de um exemplo completo com base nos t√≥picos acima, integrando EWMA, adam, SGD e Learning rate schedulers.

# Learning Rate

Bom, first things first, vamos falar sobre a learning rate (ou taxa de aprendizagem). Carinhosamente apelidado de LR, √© um hiperpar√¢metro usado no treinamento de redes neurais e em algoritmos de otimiza√ß√£o. De maneira simples: ele que define qu√£o r√°pido o modelo aprende.

De maneira mais formal: ele controla o tamanho do passo que o modelo d√° na dire√ß√£o oposta ao gradiente durante a atualiza√ß√£o dos pesos da rede. Sugerimos que se voc√™ nunca ouviu falar de learning rate, volte algumas casas e d√™ uma olhada mais aprofundada nesse conceito.

Escolher o LR ideal para o seu problema n√£o √© uma tarefa simples‚Ä¶ muitas vezes essa escolha fica na m√£o da tentativa e erro.

√â comum reduzir a taxa de aprendizado por um fator de 3 ou um fator de 10. Assim, seus valores de taxa de aprendizado poderiam ser algo como [0.1, 0.03, 0.01, 3e-3, 1e-3, 3e-4, 1e-4] (usando um fator de 3) ou [0.1, 0.01, 1e-3, 1e-4, 1e-5] (usando um fator de 10).

![Curva de perda em fun√ß√£o da Taxa de Aprendizado](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*NQE0mUfcUYB9vMagbs4OAg.png)

![Efeito de diferentes taxas de aprendizado no custo durante o treinamento](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*tFHuyFDeiPrE6Op6KXTBrg.png)

Bom, se voc√™ √© do tipo que presta aten√ß√£o aos detalhes, deve ter reparado que eu comecei comentando que o LR tamb√©m estava presente nos otimizadores.

Podemos atribuir um scheduler a um otimizador, de modo que ele atualize a taxa de aprendizado ao longo do treinamento. Vamos nos aprofundar mais nos schedulers de taxa de aprendizado daqui a alguns t√≥picos.

Dentro das possibilidades do learning rate a gente tem o Adaptive Learning Rate (Taxa de aprendizagem adaptativa), que como o nome sugere, √© dinamicamente ajustada com base na performance e no gradiente da fun√ß√£o custo.

Voc√™ j√° ouviu falar do otimizador Adam, certo? Ele √© um bom exemplo do Adaptive Learning Rate! ele come√ßa com a taxa de aprendizado fornecida como argumento, mas adapta essa taxa ao longo do treinamento, ajustando-a de forma diferente para cada par√¢metro do modelo. Ou mais ou menos isso, na verdade, o Adam n√£o adapta diretamente a taxa de aprendizado ‚Äî ele adapta os gradientes. Mas, como a atualiza√ß√£o dos par√¢metros √© feita pela multiplica√ß√£o da taxa de aprendizado e do gradiente, essa diferen√ßa se torna irrelevante na pr√°tica.

O Adam combina as caracter√≠sticas de dois outros otimizadores:

* SGD com momentum: Ele usa uma m√©dia m√≥vel dos gradientes em vez dos gradientes diretamente (isso √© conhecido como o primeiro momento em termos estat√≠sticos);
* RMSProp: Ele escala os gradientes usando uma m√©dia m√≥vel dos gradientes ao quadrado (o segundo momento, ou vari√¢ncia n√£o centralizada).
  
Complicou? Bom, vamos conhecer um pouco sobre essa m√©dia m√≥vel, ou melhor ainda, sobre a m√©dia m√≥vel exponencialmente ponderada, e depois a gente se aprofunda melhor no Adam.

# EWMA

EWMA √© uma m√©dia m√≥vel exponencialmente ponderada, e como o nome sugere, ela √© diferente da m√©dia simples‚Ä¶ Ela fornece um peso maior para valores mais recentes, se destacando na suaviza√ß√£o dos dados.

No Machine Learning, usamos o EWMA para suavizar os gradientes durante o treinamento. Pode parecer simples, mas impede que gradientes muito inst√°veis atrapalhem a converg√™ncia do modelo. Essa abordagem √© especialmente √∫til em algoritmos de otimiza√ß√£o como o Adam, que dependem de c√°lculos de m√©dias m√≥veis para ajustar a taxa de aprendizado de forma adaptativa.

Visualizar o comportamento dos gradientes suavizados com EWMA pode ajudar a entender melhor a estabilidade do treinamento e identificar poss√≠veis problemas, como gradientes explosivos ou o desaparecimento de gradientes.

Vamos entender o EWMA comparando com a m√©dia m√≥vel simples (MA).

A M√©dia M√≥vel Simples (MA) calcula a m√©dia aritm√©tica de um conjunto de valores ao longo de um per√≠odo fixo. Ela √© usada para suavizar flutua√ß√µes nos dados e destacar tend√™ncias.

Por sua vez, a M√©dia M√≥vel Exponencialmente Ponderada (EWMA) atribui pesos exponenciais decrescentes aos valores passados, dando mais import√¢ncia aos valores mais recentes.

Quando Œ± est√° pr√≥ximo de 1: os valores mais recentes recebem mais peso, resultando em uma curva mais sens√≠vel a mudan√ßas. Quando Œ± est√° pr√≥ximo de 0: os valores antigos ainda influenciam bastante, resultando em uma curva mais suave e lenta para reagir a mudan√ßas.

Vamos tentar visualizar essa diferen√ßa com um exemplo simples:

![gr√°fico EWMA](https://miro.medium.com/v2/resize:fit:786/format:webp/1*13t8PurrEYDSOMuF-IsLdw.png)

O gr√°fico compara a M√©dia M√≥vel Exponencialmente Ponderada (EWMA) e a M√©dia M√≥vel Simples (MA) usando 5 per√≠odos. O eixo Y diz respeito ao peso atribuido a cada ponto de dado anterior ao calcular a m√©dia, enquanto o eixo X representa o n√∫mero de passos (ou per√≠odos) anterires ao ponto atual (quanto maior o lag, mas distante no passado est√° o dado).

Pela imagem a gente analisa o seguinte: a EWMA atribui pesos decrescentes exponencialmente ao longo do tempo. Os valores mais recentes t√™m mais influ√™ncia no c√°lculo da m√©dia, enquanto os valores mais antigos t√™m menos peso. Isso quer dizer que a EWMA reage mais rapidamente a mudan√ßas recentes nos dados, pois d√° mais peso ao ponto atual.

J√° a MA, atribui pesos iguais para os √∫ltimos 5 per√≠odos. Assim, os √∫ltimos 5 pontos contribuem igualmente para o c√°lculo da m√©dia, enquanto os mais antigos n√£o t√™m influ√™ncia. Ou seja, quando comparado a EWMA, A MA reage mais lentamente e suaviza os dados de forma mais uniforme, sem priorizar os valores recentes.

Essa diferen√ßa √© importante no contexto de otimiza√ß√£o de redes neurais, pois a EWMA ajuda a estabilizar o treinamento ao suavizar os gradientes de forma mais din√¢mica.

# O problema do vi√©s

O Bias-Corrected EWMA (M√©dia M√≥vel Exponencialmente Ponderada Corrigida para Vi√©s) √© uma varia√ß√£o da EWMA que corrige o vi√©s introduzido no in√≠cio do c√°lculo de uma m√©dia m√≥vel exponencialmente ponderada.

Quando o c√°lculo de uma EWMA come√ßa, os primeiros valores podem estar enviesados para valores menores, pois a s√©rie temporal ainda n√£o acumulou informa√ß√µes suficientes. Isso ocorre porque o c√°lculo depende fortemente dos valores iniciais e a m√©dia tende a subestimar os valores reais no in√≠cio. Para corrigir esse vi√©s, multiplicamos o termo da EWMA por um fator de corre√ß√£o de vi√©s. Essa corre√ß√£o √© especialmente usada em algoritmos como o Adam, para melhorar a estabilidade no in√≠cio do treinamento, j√° que o otimizador utiliza EWMA tanto para os gradientes quanto para os gradientes ao quadrado.

![gr√°fico bias](https://github.com/user-attachments/assets/9b1beda2-97cc-47cf-bc5b-ddf4259ff02d)

A Figura acima mostra um exemplo pr√°tico da compara√ß√£o da M√©dia M√≥vel Simples (MA), da EWMA e a EWMA com corre√ß√£o do vi√©s (Bias). Como esperado, a EWMA sem corre√ß√£o (linha vermelha tracejada) est√° bem distante no in√≠cio, enquanto a m√©dia m√≥vel regular (linha preta tracejada) acompanha os valores reais de forma muito mais pr√≥xima. No entanto, a EWMA corrigida faz um √≥timo trabalho ao acompanhar os valores reais desde o in√≠cio. De fato, ap√≥s 19 dias, as duas EWMAs s√£o quase indistingu√≠veis.

# Adam

Voltando para o Adam, vamos entender o que tem por tr√°s. J√° sabemos que o otimizador Adam utiliza a m√©dia m√≥vel exponencialmente ponderada (EWMA) para ajustar os gradientes durante o treinamento, vamos dar uma olhada em como isso acontece.

Para cada par√¢metro do modelo, o Adam calcula duas EWMA:

* EWMA dos gradientes ‚Äî para suavizar os gradientes.
* EWMA do quadrado dos gradientes ‚Äî para escalonar os gradientes e controlar a vari√¢ncia.


Essas m√©dias s√£o corrigidas para vi√©s no in√≠cio do treinamento.

Œ≤1 e Œ≤2 s√£o hiperpar√¢metros que controlam o decaimento exponencial. Os valores t√≠picos s√£o 0.9 e 0.999.

Essa abordagem ajuda a tornar o processo de otimiza√ß√£o mais est√°vel e eficiente, especialmente para problemas complexos com gradientes ruidosos.

Vamos falar de seis argumentos para o Adam no PyTorch, a maioria j√° mencionado por aqui.

* Primeiro o params , que s√£o os par√¢metros do modelo que o otimizador ir√° atualizar durante o treinamento. Normalmente, √© passado como model.parameters() ao inicializar o otimizador.
* o lr(learning rate), a taxa de aprendizado, que controla o tamanho dos passos na dire√ß√£o do gradiente durante a atualiza√ß√£o dos par√¢metros. Valor padr√£o: 1e-3 (0.001).
* beta1: controla a suaviza√ß√£o dos gradientes.
* beta2: controla a suaviza√ß√£o dos gradientes ao quadrado (para a escala).
* eps(epsilon), um pequeno valor constante (normalmente 1e-8) adicionado ao denominador para evitar divis√£o por zero e garantir estabilidade num√©rica.

Esses quatro argumentos s√£o os principais para o funcionamento b√°sico do Adam, enquanto weight_decay e amsgrad s√£o argumentos opcionais que controlam regulariza√ß√£o e varia√ß√µes do algoritmo.

* weight_decay vai adicionar uma regulariza√ß√£o L2 para prevenir overfitting, reduzindo o valor dos pesos ao longo do tempo.
* amsgrad √© uma variante do Adam que ajusta a forma de calcular o gradiente adaptado para maior estabilidade.
  
Esses ajustes tornam o Adam mais robusto e controlado, especialmente em treinamentos de redes neurais profundas.

# Visualizando os Gradientes Adaptativos

Agora que a gente entende melhor os gradientes, que tal gerar uma vizualiza√ß√£o? Os c√≥digos utilizados para gerar as visualiza√ß√µes s√£o do livro Deep Learning with PyTorch Step-by-Step. Apesar do livro possuir seu pr√≥prio reposit√≥rio no Github, o c√≥digo comentado passo a passo pode ser visto [aqui](https://github.com/ElisaGabriela/Machine-Learning/blob/main/Tarefa%2003/Tarefa3.ipynb).


Vamos usar um problema simples de regress√£o linear, executar o loop de treinamento para que possamos registrar os gradientes. Vamos ilustrar os efeitos de diferentes par√¢metros na minimiza√ß√£o da perda.

![image](https://github.com/user-attachments/assets/816b7623-3427-4f80-9a8b-f198ca8cb32c)


No gr√°fico √† esquerda, vemos que o EWMA corrigido por vi√©s dos gradientes (em vermelho) suaviza os gradientes. No centro, o EWMA corrigido por vi√©s dos gradientes ao quadrado √© usado para escalar os gradientes suavizados. √Ä direita, ambos os EWMAs s√£o combinados para calcular os gradientes adaptados.

Internamente, o Adam mant√©m dois valores para cada par√¢metro, exp_avg e exp_avg_sq, representando os EWMAs (n√£o corrigidos) para gradientes e gradientes ao quadrado, respectivamente.


Agora que tal comparar com o SGD? Temos discutido como a atualiza√ß√£o do par√¢metro √© diferente, mas agora √© hora de mostrar como isso afeta o treinamento do modelo. Vamos visualizar o caminho percorrido por cada otimizador para trazer os dois par√¢metros (mais pr√≥ximos) de seus valores √≥timos.


![image](https://github.com/user-attachments/assets/450029c2-8f84-4a92-ace7-03aab140ce27)

No gr√°fico √† esquerda, temos o caminho t√≠pico e bem comportado (e lento) percorrido pelo SGD. Voc√™ pode ver que ele oscila um pouco devido ao ru√≠do introduzido pelo uso de mini-lotes. No gr√°fico √† direita, vemos o efeito do uso das m√©dias m√≥veis exponenciais: por um lado, √© mais suave e se move mais r√°pido; por outro, ele ultrapassa o alvo e precisa mudar de dire√ß√£o v√°rias vezes enquanto se aproxima do objetivo. Est√° se adaptando √† superf√≠cie de perda, por assim dizer.

Falando em perdas, tamb√©m podemos comparar as trajet√≥rias das perdas de treinamento e valida√ß√£o para cada otimizador.


![image](https://github.com/user-attachments/assets/5b1da362-8c33-471b-b9b6-a27ae1290ada)

Lembre-se, as perdas s√£o calculadas ao final de cada √©poca, com base na m√©dia das perdas dos mini-lotes. No gr√°fico √† esquerda, mesmo que o SGD oscile um pouco, podemos ver que cada √©poca apresenta uma perda menor que a anterior. No gr√°fico √† direita, o overshooting fica claramente vis√≠vel como um aumento na perda de treinamento. Mas tamb√©m √© evidente que o Adam alcan√ßa uma perda menor, pois se aproximou mais do valor √≥timo (o ponto vermelho no gr√°fico anterior).

Em problemas reais, onde √© praticamente imposs√≠vel tra√ßar a superf√≠cie de perda, podemos olhar para as perdas como um ‚Äúresumo executivo‚Äù do que est√° acontecendo. Perdas de treinamento √†s vezes podem aumentar antes de cair novamente, e isso √© esperado.

# Stochastic Gradient Descent (SGD) 

Bora falar um pouqinho sobre SGD‚Ä¶O Gradiente Descendente Estoc√°stico A.K.A SGD √© um algoritmo de otimiza√ß√£o que atualiza os pesos de um modelo utilizando uma amostra aleat√≥ria dos dados de treinamento. O SGD do PyTorch tem alguns argumentos:

* params: par√¢metros do modelo
* lr: taxa de aprendizado
* weight_decay: penalidade L2
* momentum: fator de momentum, o pr√≥prio argumento beta do SGD.
* dampening: fator de amortecimento para o momentum
* nesterov: habilita o momentum de Nesterov, que √© uma vers√£o mais inteligente do momentum regular.

## SGD com Momentum

O SGD com Momentum √© uma varia√ß√£o do SGD, que busca acelerar a converg√™ncia e reduzir as oscila√ß√µes durante o processo de otimiza√ß√£o. Inspirado no conceito de f√≠sica, onde um objeto em movimento tende a continuar sua trajet√≥ria com base na velocidade adquirida, o SGD com Momentum utiliza o gradiente acumulado das itera√ß√µes anteriores para atualizar os par√¢metros, ao inv√©s de se basear apenas no gradiente atual.

Apesar de parecer semelhante ao uso de uma m√©dia exponencialmente ponderada (EWMA) para os gradientes, o SGD com Momentum n√£o faz uma m√©dia, mas sim uma soma cumulativa de gradientes ‚Äúdescontados‚Äù. Gradientes passados contribuem para a soma, mas s√£o progressivamente ‚Äúdescontados‚Äù √† medida que envelhecem, e esse desconto √© controlado pelo fator beta. Esse fator de amortecimento, comumente configurado em 0.9, determina o impacto do ‚Äúpassado‚Äù nas atualiza√ß√µes. O gradiente mais recente tem sua contribui√ß√£o reduzida pelo fator de amortecimento, enquanto os gradientes anteriores influenciam cada vez menos √† medida que se tornam mais distantes.

No entanto, apesar de gradientes antigos contribu√≠rem de forma decrescente, gradientes recentes ainda t√™m grande influ√™ncia. Isso resulta em um comportamento em que o algoritmo tende a fazer atualiza√ß√µes r√°pidas, e pode acabar ‚Äúpassando do ponto‚Äù ao se mover muito r√°pido, o que exige ajustes ao longo do caminho para corrigir o curso. Esse efeito pode ser visualizado como uma bola rolando por uma colina, ganhando velocidade at√© ultrapassar o ponto de m√≠nimo e precisando recuar para alcan√ßar o objetivo.

![image](https://github.com/user-attachments/assets/39124a5e-caf9-4b8a-a37e-4a41302d68bf)

Por sua vez, o ADAM incorpora n√£o apenas o momento (m√©dia do gradiente) como o SGD com Momentum, mas tamb√©m uma m√©dia m√≥vel do quadrado do gradiente. Isso permite que o ADAM adapte dinamicamente as taxas de aprendizado para cada par√¢metro, proporcionando uma abordagem mais robusta e eficaz em situa√ß√µes de grande variabilidade nos gradientes. No entanto, essa capacidade de adapta√ß√£o do ADAM pode resultar em uma converg√™ncia mais r√°pida para um m√≠nimo, por√©m nem sempre para um m√≠nimo de boa qualidade, especialmente em problemas de aprendizado profundo, onde diferentes m√≠nimos podem ter desempenhos variados.

Embora o ADAM seja eficaz e frequentemente mais est√°vel, o SGD com Momentum tem o potencial de explorar melhor a superf√≠cie de perda, especialmente quando combinado com um learning rate scheduler. Enquanto o ADAM tende a encontrar m√≠nimos rapidamente, o Momentum, com sua ‚Äúoscila√ß√£o‚Äù controlada, pode levar a um m√≠nimo de melhor qualidade. Em resumo, ambos os algoritmos t√™m como objetivo melhorar a converg√™ncia, mas o ADAM oferece um desempenho mais consistente e requer menos ajuste de par√¢metros, enquanto o SGD com Momentum pode ser preferido em contextos onde se deseja um controle mais refinado sobre o processo de otimiza√ß√£o.

## SGD com Nesterov

O Nesterov Momentum √© uma varia√ß√£o do m√©todo de momentum que introduz uma t√©cnica de ‚Äúolhar √† frente‚Äù, antecipando a dire√ß√£o do movimento para melhorar a converg√™ncia. Em vez de calcular o gradiente no ponto atual, o m√©todo de Nesterov calcula o gradiente ap√≥s o passo do momentum, ou seja, faz uma previs√£o de onde o ponto de atualiza√ß√£o estar√° no pr√≥ximo passo, ajustando a dire√ß√£o de forma mais eficiente. No SGD com Momentum tradicional, no passo t, o momentum √© calculado com base no gradiente do passo t e no momentum do passo t‚àí1. Para o pr√≥ximo passo t+1, o algoritmo utiliza o gradiente no ponto t+1 e o momentum calculado no passo anterior para atualizar os par√¢metros. Em contraste, o Nesterov faz uma previs√£o mais inteligente: antes de atualizar o par√¢metro no passo t, o algoritmo ‚Äúantecipa‚Äù a atualiza√ß√£o e calcula o gradiente n√£o no ponto t, mas em um ponto projetado para t+1. Isso √© feito utilizando o gradiente atual e o momentum acumulado at√© o momento, considerando que o gradiente no pr√≥ximo passo t+1 ser√° uma estimativa do gradiente no passo atual t.


![image](https://github.com/user-attachments/assets/19c6c298-10d3-436d-b1ff-b504442c9cbe)

Em termos pr√°ticos, o Nesterov Momentum tenta calcular o momentum um passo √† frente, antecipando a dire√ß√£o em que o gradiente provavelmente se mover√°. Isso √© poss√≠vel porque, embora o gradiente no pr√≥ximo passo n√£o seja conhecido, uma estimativa razo√°vel √© usar o gradiente no ponto atual, dada a suposi√ß√£o de que o movimento entre os pontos n√£o mudar√° drasticamente. Ao calcular o momentum antecipadamente e us√°-lo para atualizar os par√¢metros, o m√©todo melhora a velocidade de converg√™ncia, j√° que os par√¢metros s√£o ajustados de forma mais precisa, refletindo melhor a dire√ß√£o desejada.

Em resumo, a principal diferen√ßa do Nesterov Momentum √© a forma como ele calcula o gradiente, n√£o apenas considerando o ponto atual, mas tamb√©m antecipando o movimento. Isso permite que o Nesterov se mova mais rapidamente em dire√ß√£o ao m√≠nimo, pois ele j√° se ajusta ao futuro comportamento do gradiente. Os gradientes, embora calculados da mesma forma em todas as varia√ß√µes do momentum, resultam em diferentes trajet√≥rias de atualiza√ß√£o, uma vez que a posi√ß√£o na superf√≠cie de perda onde o gradiente √© calculado varia.


![image](https://github.com/user-attachments/assets/7707e15f-0e0d-41c9-8853-14992a81623f)

Essa abordagem de ‚Äúolhar √† frente‚Äù pode ser especialmente √∫til em modelos de aprendizado profundo, onde a superf√≠cie de perda √© complexa e pode ter muitos m√≠nimos locais. Ao antecipar o gradiente de forma mais inteligente, o Nesterov Momentum pode escapar de m√≠nimos locais e alcan√ßar um m√≠nimo de melhor qualidade mais rapidamente.

![image](https://github.com/user-attachments/assets/8ea6c83b-fe17-418b-a8e0-153eb8b3da7e)

Se voc√™ ainda n√£o est√° convencido pelo Momentum, seja o regular ou o de Nesterov, podemos adicionar um outro fator √† discuss√£o: o uso de um learning rate scheduler, que pode tornar o processo ainda mais eficaz ao ajustar dinamicamente a taxa de aprendizado durante a otimiza√ß√£o.

# Learning Rate Schedulers 

Este √© um tema muito amplo e repleto de novos conceitos. Vamos apenas introduzir os agendadores de taxa de aprendizado (tradu√ß√£o de learning rate schedulers).

√â poss√≠vel programar mudan√ßas na taxa de aprendizado durante o treinamento, em vez de apenas adaptar os gradientes. Por exemplo, voc√™ pode querer reduzir a taxa de aprendizado em uma ordem de magnitude a cada T √©pocas, de forma que o treinamento seja mais r√°pido no in√≠cio e desacelere ap√≥s algum tempo, para evitar problemas de converg√™ncia.

Isso √© exatamente o que um Learning Rate Scheduler (agendador de taxa de aprendizado) faz: ele atualiza a taxa de aprendizado do otimizador.

Sendo mais clara: a taxa de aprendizado inicial √© mantida por algumas √©pocas e depois √© multiplicada por um fator (ex.: 0.1). Isso permite que o treinamento comece r√°pido e desacelere com o tempo para evitar problemas de converg√™ncia.

‚ÄúTodos os agendadores reduzem a taxa de aprendizado?‚Äù

N√£o, nem todos. Antigamente, era comum reduzir a taxa de aprendizado ao longo do treinamento, mas essa ideia foi desafiada por taxas c√≠clicas (cyclical learning rates). Existem v√°rias abordagens de agendamento, e muitas delas est√£o dispon√≠veis no PyTorch.

Dividimos os agendadores em tr√™s grupos:

* Agendadores que atualizam a taxa a cada T √©pocas (como no exemplo acima).
* Agendadores que atualizam a taxa quando a validation loss parece estagnada.
* Agendadores que atualizam a taxa ap√≥s cada mini-batch.

## Epoch Schedulers
Estes agendadores t√™m o m√©todo step() chamado no final de cada √©poca.

* StepLR: Multiplica a taxa por um fator (gamma) a cada step_size √©pocas.
* MultiStepLR: Multiplica a taxa por gamma nas √©pocas especificadas em uma lista de milestones.
* ExponentialLR: Multiplica a taxa por gamma em todas as √©pocas.
* LambdaLR: Permite personalizar a atualiza√ß√£o com uma fun√ß√£o que recebe a √©poca como argumento.
* CosineAnnealingLR: Usa cosine annealing para atualizar a taxa.


![image](https://github.com/user-attachments/assets/14d34279-9540-4fd2-ac8d-a4a7bc601021)

## Validation Loss Scheduler

O agendador ReduceLROnPlateau n√£o segue um cronograma fixo. Em vez disso, reduz a taxa de aprendizado quando a perda de valida√ß√£o n√£o melhora por um n√∫mero definido de √©pocas (argumento patience). Por exemplo, se a perda estagnar por 5 √©pocas, a taxa ser√° reduzida na pr√≥xima.


![image](https://github.com/user-attachments/assets/c8899651-81c7-4c94-bec0-1fde9dbcd4c0)

## Mini-Batch Schedulers

Esses t√™m o m√©todo step() chamado ap√≥s cada mini-batch.

* CyclicLR: Alterna entre uma taxa m√≠nima (base_lr) e m√°xima (max_lr) ao longo de ciclos. Modos como triangular2 ou exp_range permitem ajustar a amplitude dos ciclos.
* OneCycleLR: Aumenta a taxa at√© um valor m√°ximo e a reduz para um valor bem baixo em um √∫nico ciclo.
* CosineAnnealingWarmRestarts: Requer o n√∫mero da √©poca (incluindo fra√ß√µes de mini-batches) para aplicar cosine annealing.


  ![image](https://github.com/user-attachments/assets/ab1d9c1e-9a33-4165-b629-909677e38ab5)

# Hora da pr√°tica ‚è∞

Vamos integrar todos os conceitos que vimos hoje: EWMA, adam, SDG e learning rate schedulers.

O fluxo de treinamento vai ser o seguinte:

* Pr√©-processamento dos dados: Normaliza√ß√£o e prepara√ß√£o dos loaders.
* Inicializa√ß√£o do modelo: Defini√ß√£o da arquitetura e escolha do otimizador (Adam ou SGD com momentum).
* Configura√ß√£o do scheduler: Definir um learning rate scheduler para ajuste din√¢mico da taxa de aprendizado.
* Treinamento: Atualizar pesos com o otimizador, aplicar EWMA nos gradientes e monitorar a performance.
* Avalia√ß√£o: Calcular acur√°cia e perda no conjunto de valida√ß√£o.
  
O livro do Godoy tr√°s uma an√°lise muito semelhante na se√ß√£o ‚ÄúPutting It All Together‚Äù, vale a pena dar uma olhada! Tentei construir um exemplo mais did√°tico, ent√£o vamos l√°‚Ä¶

O primeiro passo √© escolher nosso dataset. Se queremos um exemplo de CNN simples, nada melhor do que usar o MNIST, um dos datasets mais usados para exemplos de vis√£o computacional.

O MNIST √© um grande conjunto de dados (treinamento com 60.000 exemplos e um conjunto de teste com 10.000 exemplos) de d√≠gitos manuscritos. Os d√≠gitos foram normalizados em termos de tamanho e centralizados em uma imagem de tamanho fixo.


![image](https://github.com/user-attachments/assets/f17aae6c-d433-4ca0-8a5a-9c1731926142)

Vamos utilizar o DataLoader e uma normaliza√ß√£o baseada em estat√≠sticas do dataset.


    ~~~python
    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
    train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    ~~~

O nosso pr√≥ximo passo √© a inicializa√ß√£o do nosso modelo. Como o foco desta se√ß√£o √© mostra o uso das ferramentas, vamos aplicar uma arquitetura bem simples.


    ~~~python
    class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.model = nn.Sequential(
            nn.Flatten(),
            nn.Linear(28*28, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 10)
        )
    
    def forward(self, x):
        return self.model(x)
    ~~~

Agora vamos para o que interessa! Vamos definir o Adam e o SGD para compara√ß√£o e incluir um scheduler da nossa escolha (aqui d√° para brincar e testar todos que estudamos).


    ~~~python
    model = SimpleNN()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
    ~~~

N√£o podemos esquecer da suaviza√ß√£o dos gradientes com EWMA.


    ~~~python
    def ewma_gradients(model, beta=0.9):
    avg_grads = {}
    for name, param in model.named_parameters():
        if param.grad is not None:
            if name not in avg_grads:
                avg_grads[name] = param.grad.clone()
            else:
                avg_grads[name] = beta * avg_grads[name] + (1 - beta) * param.grad
    return avg_grads
    ~~~

Agora vamos treinar nosso modelo e obter nossas acur√°cias para cada √©poca.

    ~~~python
    num_epochs = 20
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        
        for batch in train_loader:
            inputs, targets = batch
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = nn.CrossEntropyLoss()(outputs, targets)
            loss.backward()
            avg_grads = ewma_gradients(model)
            optimizer.step()
        
        scheduler.step()
        print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")
    ~~~

Bom, com isso j√° temos um modelo ‚Äî mesmo que simples ‚Äî em pleno funcionamento! Vamos ver como ele ele foi na pr√°tica?

Primeiro, vamos ver nossa curva de acur√°cia!

![image](https://github.com/user-attachments/assets/e9c98057-7afe-4317-b417-c61527bd744f)

Para um modelo simples, os resultados foram bem bacanas! 97,42% de acur√°cia no treinamento :)

Vamos fazer mais algumas an√°lises‚Ä¶ que tal dar uma olhada na evolu√ß√£o da taxa de aprendizado?

![image](https://github.com/user-attachments/assets/aa579b6f-8521-47be-a636-3b7967e0c588)

A gente consegue ver que a taxa de aprendizado come√ßa relativamente alta e decresce em etapas ao longo das √©pocas. Isso se deve ao scheduler escolhido, que reduz a taxa de aprendizado progressivamente, o que pode ajudar o modelo a refinar os pesos ao longo do treinamento, prevenindo oscila√ß√µes excessivas no final. Podemos testar outros tipos de schedulers para comparar seu comportamento. Legal, n√©?

E o EWMA? Vamos ver se a suaviza√ß√£o funcionou? Para isso podemos fazer um plot comparando os gradientes ‚Äúcrus‚Äù e a suaviza√ß√£o.

![image](https://github.com/user-attachments/assets/7641c43a-c7cc-41be-8a46-3c49d4e7cd06)

Inicialmente, os gradientes s√£o elevados, mas reduzem com o tempo, indicando que o modelo est√° se ajustando e atualizando os pesos de forma mais sutil ao longo do tempo. A curva suavizada mostra uma diminui√ß√£o mais est√°vel dos gradientes, o que sugere um processo de converg√™ncia controlado, do jeito que a gente queria ü•≥ü™©.

# Refer√™ncias
* Deep Learning with PyTorch Step-by-Step ‚Äî Daniel Godoy
* GitHub do livro: https://github.com/dvgodoy/PyTorchStepByStep
* https://medium.com/aimonks/understanding-exponentially-weighted-moving-averages-ewma-cc8e78b01184
* GitHub da disciplina de Machine Learning do PPGEEC: https://github.com/ivanovitchm/PPGEEC2318
* https://www.datacamp.com/pt/tutorial/adam-optimizer-tutorial
* https://www.geeksforgeeks.org/impact-of-learning-rate-on-a-model/

