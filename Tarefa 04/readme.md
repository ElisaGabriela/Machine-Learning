# Transfer Learning

Este projeto explora a aplica√ß√£o de Transfer Learning utilizando a rede neural convolucional VGG16 no dataset Animals-10. Foram testados dois m√©todos principais:

* Feature Extraction: Utilizando as camadas convolucionais do VGG16 como extrator de features, mantendo os pesos congelados e treinando apenas a camada de classifica√ß√£o final.
* Fine-Tuning: Ajustando os pesos de algumas camadas convolucionais superiores do VGG16 para melhorar a capacidade de generaliza√ß√£o do modelo.

Para visualizar o passo a passo das execu√ß√µes dos m√©todos acesse a nota do medium aqui : [üßæ](https://medium.com/@elisa.lucena.127/tranfer-learning-como-reciclar-modelos-de-ml-%EF%B8%8F-bdb11f0907e2)
## Dataset
O dataset utilizado foi o Animals-10, dispon√≠vel no Kaggle, que cont√©m aproximadamente 28 mil imagens de 10 categorias de animais:

* C√£o
* Gato
* Cavalo
* Aranha
* Borboleta
* Galinha
* Ovelha
* Vaca
* Esquilo
* Elefante

![img](https://cdn-images-1.medium.com/max/1100/1*W3e-jXTWFtwwW2fArKgasg.png)

## Pr√©-processamento de dados

As imagens do dataset apresentam diferentes tamanhos e resolu√ß√µes, o que pode afetar o desempenho do modelo. Para padronizar os dados, redimensionamos todas as imagens para 224x224 pixels, que √© o tamanho esperado pelo modelo VGG16. Al√©m disso, normalizamos os valores dos pixels para a faixa [0,1] e aplicamos a padroniza√ß√£o com a m√©dia e desvio padr√£o do ImageNet, garantindo compatibilidade com os pesos pr√©-treinados.

Vamos usar t√©cnicas de data augmentation para aumentar a variabilidade do conjunto de treino. Esse conjunto de t√©cnicas melhoram o desempenho dos modelos, aproveitando ao m√°ximo os dados dispon√≠veis. Evita overfitting, melhora a acur√°cia e aumenta a diversidade dos dados de treinamento. 

As t√©cnicas utilizadas s√£o: rota√ß√£o, deslocamento, zoom e espelhamento.

## VGG16

![img](https://media.geeksforgeeks.org/wp-content/uploads/20200219152207/new41.jpg)

## Feature Extraction üëΩ

Feature extraction √© uma t√©cnica de transfer learning que utiliza os pesos de uma rede neural convolucional pr√©-treinada para extrair padr√µes relevantes das imagens.

A principal caracter√≠stica desse m√©todo √© congelar as camadas convolucionais e substituir a camada final da rede. Com isso, √© poss√≠vel reutilizar as features aprendidas em um novo conjunto de dados.

![img](https://cdn-images-1.medium.com/max/1100/0*vOQvqcaiyAA9Wz6T.jpg)

Os resultados obtidos podem ser vistos na figura abaixo:

![img](https://cdn-images-1.medium.com/max/1100/1*d6wYzV0VmhVMz87YrrJnwA.png)

## Fine-Tuning üìè

Enquanto o Feature Extraction congela as camadas convolucionais da rede pr√©-treinada e substitui apenas a camada final para adapta√ß√£o ao novo conjunto de dados, o Fine-Tuning descongela algumas camadas convolucionais (geralmente as √∫ltimas) e permite que elas sejam atualizadas durante o treinamento, possibilitando um ajuste fino da rede no novo dom√≠nio.

![img](https://cdn-images-1.medium.com/max/1100/0*DiUp4XsDfMEdQTth.jpg)

Os resultados obtidos podem ser vistos na figura abaixo:

![img](https://cdn-images-1.medium.com/max/1100/1*Y8JOEUAcxUqf0Gp5dblU0A.png)

## Escolha do m√©todo

A decis√£o entre fine-tuning e feature extraction depende da complexidade da nova tarefa, do tamanho do conjunto de dados dispon√≠vel e da similaridade entre as tarefas pr√©via e atual. Em geral, o fine-tuning tende a oferecer melhor desempenho quando h√° dados suficientes dispon√≠veis, enquanto o Feature Extraction √© mais apropriado para conjuntos de dados menores ou quando o treinamento do modelo completo √© computacionalmente custoso.

![img](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*PjFqIsF5KA6vTyEpnQm4Vg.jpeg)





